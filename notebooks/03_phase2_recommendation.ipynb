{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853eb9ca-8d75-4d8f-8667-5bab84c77b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# PHASE 2 - RECOMMENDATION SYSTEMS\n",
    "# ================================================================\n",
    "# \n",
    "# Goal: Build hybrid recommendation system combining:\n",
    "#   - Collaborative Filtering (user-item interactions)\n",
    "#   - Content-Based Filtering (product features)\n",
    "#   - Segment-aware recommendations\n",
    "#\n",
    "# Sections:\n",
    "#   2.0 - Setup & Data Loading\n",
    "#   2.1 - Collaborative Filtering (CF)\n",
    "#   2.2 - Content-Based Filtering (CBF)\n",
    "#   2.3 - Hybrid Recommendations\n",
    "#   2.4 - Evaluation\n",
    "#   2.5 - Segment-Specific Recommendations\n",
    "# ================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d7b44-8ef1-4bbf-93a0-1c26768f1613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHASE 2 - RECOMMENDATION SYSTEMS\n",
      "============================================================\n",
      "\n",
      "Loading temporal split data from Phase 0...\n",
      " Loaded temporal splits:\n",
      "   Train: 2,880,077 orders, 29,014,490 items\n",
      "   Val:   175,072 orders, 1,830,111 items\n",
      "   Test:  175,072 orders, 1,861,372 items\n",
      "\n",
      "Loading cluster assignments from Phase 1...\n",
      " Loaded cluster assignments for 175,072 users\n",
      "   Clusters: 5\n",
      "   Segment names: ['Routine Snackers', 'Power Users', 'Bulk Shoppers', 'Household Essentials', 'Alcohol Enthusiasts']\n",
      "\n",
      "Merging cluster information with temporal splits...\n",
      " Cluster info merged with orders\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 2.0 - Setup & Data Loading\n",
    "# ================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "SEED = 000\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 2 - RECOMMENDATION SYSTEMS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Load Phase 0 Data (Temporal Split)\n",
    "# -----------------------------------------------\n",
    "print(\"Loading temporal split data from Phase 0...\")\n",
    "\n",
    "orders_train = pd.read_parquet('../data/processed/orders_train.parquet')\n",
    "orders_val = pd.read_parquet('../data/processed/orders_val.parquet')\n",
    "orders_test = pd.read_parquet('../data/processed/orders_test.parquet')\n",
    "\n",
    "order_products_train = pd.read_parquet('../data/processed/order_products_train.parquet')\n",
    "order_products_val = pd.read_parquet('../data/processed/order_products_val.parquet')\n",
    "order_products_test = pd.read_parquet('../data/processed/order_products_test.parquet')\n",
    "\n",
    "products = pd.read_parquet('../data/processed/products.parquet')\n",
    "departments = pd.read_parquet('../data/processed/departments.parquet')\n",
    "aisles = pd.read_parquet('../data/processed/aisles.parquet')\n",
    "\n",
    "print(f\" Loaded temporal splits:\")\n",
    "print(f\"   Train: {len(orders_train):,} orders, {len(order_products_train):,} items\")\n",
    "print(f\"   Val:   {len(orders_val):,} orders, {len(order_products_val):,} items\")\n",
    "print(f\"   Test:  {len(orders_test):,} orders, {len(order_products_test):,} items\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Load Phase 1 Data (Clusters)\n",
    "# -----------------------------------------------\n",
    "print(\"\\nLoading cluster assignments from Phase 1...\")\n",
    "\n",
    "user_features_clustered = pd.read_parquet('../data/processed/user_features_raw_clustered.parquet')\n",
    "\n",
    "print(f\" Loaded cluster assignments for {len(user_features_clustered):,} users\")\n",
    "print(f\"   Clusters: {user_features_clustered['cluster'].nunique()}\")\n",
    "print(f\"   Segment names: {user_features_clustered['segment_name'].unique().tolist()}\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Merge cluster info with orders\n",
    "# -----------------------------------------------\n",
    "print(\"\\nMerging cluster information with temporal splits...\")\n",
    "\n",
    "orders_train = orders_train.merge(\n",
    "    user_features_clustered[['user_id', 'cluster', 'segment_name']], \n",
    "    on='user_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "orders_val = orders_val.merge(\n",
    "    user_features_clustered[['user_id', 'cluster', 'segment_name']], \n",
    "    on='user_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "orders_test = orders_test.merge(\n",
    "    user_features_clustered[['user_id', 'cluster', 'segment_name']], \n",
    "    on='user_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\" Cluster info merged with orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e00141e-b341-4c24-9e14-901fec2ecfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2.1 - Collaborative Filtering\n",
      "============================================================\n",
      "\n",
      " Surprise library imported\n",
      "\n",
      "Preparing user-item interaction data...\n",
      "Training interactions: 29,014,490\n",
      "Unique users: 175,072\n",
      "Unique products: 49,623\n",
      "\n",
      "Rating statistics:\n",
      "  Min: 0.693\n",
      "  Max: 4.595\n",
      "  Mean: 1.040\n",
      "  Median: 0.693\n",
      "\n",
      "Converting to Surprise Dataset format...\n",
      " Trainset created:\n",
      "   Users: 175,072\n",
      "   Items: 49,623\n",
      "   Ratings: 11,629,304\n",
      "   Sparsity: 0.9987\n",
      "\n",
      "------------------------------------------------------------\n",
      "Training SVD (Matrix Factorization) Model\n",
      "------------------------------------------------------------\n",
      " SVD model trained\n",
      "\n",
      "------------------------------------------------------------\n",
      "Creating Recommendation Functions\n",
      "------------------------------------------------------------\n",
      " Recommendation functions created:\n",
      " - get_top_n_recommendations\n",
      " - get_recommendations_for_users\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing Recommendations on Sample Users\n",
      "------------------------------------------------------------\n",
      "\n",
      "SVD Recommendations:\n",
      "----------------------------------------\n",
      "\n",
      "User 132637:\n",
      "  Past purchases: 28 products\n",
      "  Top 5 recommendations:\n",
      "    1. Half And Half Ultra Pasteurized (score: 2.584)\n",
      "    2. Organic Lactose Free Whole Milk (score: 2.339)\n",
      "    3. 1% Milkfat Low Fat Vitamin A & D Milk (score: 2.317)\n",
      "    4. Organic Reduced Fat Milk (score: 2.305)\n",
      "    5. 0% Greek Strained Yogurt (score: 2.286)\n",
      "\n",
      "User 160970:\n",
      "  Past purchases: 52 products\n",
      "  Top 5 recommendations:\n",
      "    1. Organic Low Fat Milk (score: 2.602)\n",
      "    2. Goat Milk (score: 2.451)\n",
      "    3. Organic Whole Milk with DHA Omega-3 (score: 2.337)\n",
      "    4. Organic Homogenized Whole Milk (score: 2.325)\n",
      "    5. Organic Grade A Raw Whole Milk (score: 2.308)\n",
      "\n",
      "User 151132:\n",
      "  Past purchases: 80 products\n",
      "  Top 5 recommendations:\n",
      "    1. Purified Alkalkine Water with Minerals pH10 (score: 1.734)\n",
      "    2. Whole Vitamin D Milk (score: 1.718)\n",
      "    3. Whole Organic Omega 3 Milk (score: 1.695)\n",
      "    4. Pure Sparkling Water (score: 1.689)\n",
      "    5. Goat Milk (score: 1.686)\n",
      "\n",
      "============================================================\n",
      "Section 2.1 Complete - CF Models Trained\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 2.1 - Collaborative Filtering (CF)\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2.1 - Collaborative Filtering\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Install surprise if needed\n",
    "# -----------------------------------------------\n",
    "\n",
    "from surprise import Dataset, Reader, SVD\n",
    "\n",
    "print(\" Surprise library imported\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Prepare data for Surprise\n",
    "# -----------------------------------------------\n",
    "print(\"\\nPreparing user-item interaction data...\")\n",
    "\n",
    "# Merge train orders with products\n",
    "train_interactions = order_products_train.merge(\n",
    "    orders_train[['order_id', 'user_id']], \n",
    "    on='order_id'\n",
    ")\n",
    "\n",
    "print(f\"Training interactions: {len(train_interactions):,}\")\n",
    "print(f\"Unique users: {train_interactions['user_id'].nunique():,}\")\n",
    "print(f\"Unique products: {train_interactions['product_id'].nunique():,}\")\n",
    "\n",
    "# Create rating-like data\n",
    "# Use log-transformed purchase frequency\n",
    "purchase_counts = train_interactions.groupby(['user_id', 'product_id']).size().reset_index(name='frequency')\n",
    "purchase_counts['rating'] = np.log1p(purchase_counts['frequency'])  # log(1 + freq)\n",
    "\n",
    "cf_data = purchase_counts[['user_id', 'product_id', 'rating']].copy()\n",
    "\n",
    "print(f\"\\nRating statistics:\")\n",
    "print(f\"  Min: {cf_data['rating'].min():.3f}\")\n",
    "print(f\"  Max: {cf_data['rating'].max():.3f}\")\n",
    "print(f\"  Mean: {cf_data['rating'].mean():.3f}\")\n",
    "print(f\"  Median: {cf_data['rating'].median():.3f}\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Convert to Surprise format\n",
    "# -----------------------------------------------\n",
    "print(\"\\nConverting to Surprise Dataset format...\")\n",
    "\n",
    "# Train SVD with defaults\n",
    "reader = Reader(rating_scale=(cf_data['rating'].min(), cf_data['rating'].max()))\n",
    "\n",
    "# Load data\n",
    "surprise_data = Dataset.load_from_df(cf_data, reader)\n",
    "\n",
    "# Build full trainset\n",
    "trainset = surprise_data.build_full_trainset()\n",
    "\n",
    "print(f\" Trainset created:\")\n",
    "print(f\"   Users: {trainset.n_users:,}\")\n",
    "print(f\"   Items: {trainset.n_items:,}\")\n",
    "print(f\"   Ratings: {trainset.n_ratings:,}\")\n",
    "print(f\"   Sparsity: {1 - (trainset.n_ratings / (trainset.n_users * trainset.n_items)):.4f}\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Train SVD Model\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Training SVD (Matrix Factorization) Model\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "svd_model = SVD(random_state=SEED)  # All defaults\n",
    "svd_model.fit(trainset)\n",
    "\n",
    "print(\" SVD model trained\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Generate Recommendations Function\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Creating Recommendation Functions\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "def get_top_n_recommendations(model, user_id, n=10, exclude_purchased=True):\n",
    "    \"\"\"\n",
    "    Get top N product recommendations for a user\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Surprise model (SVD)\n",
    "        user_id: Target user ID\n",
    "        n: Number of recommendations\n",
    "        exclude_purchased: Whether to exclude already purchased items\n",
    "        \n",
    "    Returns:\n",
    "        List of (product_id, predicted_rating) tuples\n",
    "    \"\"\"\n",
    "    # Get all product IDs\n",
    "    all_products = cf_data['product_id'].unique()\n",
    "    \n",
    "    # Get products user has already purchased\n",
    "    if exclude_purchased:\n",
    "        purchased = cf_data[cf_data['user_id'] == user_id]['product_id'].unique()\n",
    "        candidate_products = [p for p in all_products if p not in purchased]\n",
    "    else:\n",
    "        candidate_products = all_products\n",
    "    \n",
    "    # Predict ratings for all candidate products\n",
    "    predictions = []\n",
    "    for product_id in candidate_products:\n",
    "        pred = model.predict(user_id, product_id)\n",
    "        predictions.append((product_id, pred.est))\n",
    "    \n",
    "    # Sort by predicted rating (descending)\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return predictions[:n]\n",
    "\n",
    "def get_recommendations_for_users(model, user_ids, n=10):\n",
    "    \"\"\"\n",
    "    Get recommendations for multiple users\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary: {user_id: [(product_id, score), ...]}\n",
    "    \"\"\"\n",
    "    recommendations = {}\n",
    "    for user_id in user_ids:\n",
    "        recs = get_top_n_recommendations(model, user_id, n=n)\n",
    "        recommendations[user_id] = recs\n",
    "    return recommendations\n",
    "\n",
    "print(\" Recommendation functions created:\")\n",
    "print(\" - get_top_n_recommendations\")\n",
    "print(\" - get_recommendations_for_users\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Test Recommendations\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Testing Recommendations on Sample Users\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Sample 3 random users\n",
    "sample_users = cf_data['user_id'].sample(3, random_state=SEED).tolist()\n",
    "\n",
    "print(f\"\\nSVD Recommendations:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for user_id in sample_users:\n",
    "    recs = get_top_n_recommendations(svd_model, user_id, n=5)\n",
    "    \n",
    "    print(f\"\\nUser {user_id}:\")\n",
    "    print(f\"  Past purchases: {cf_data[cf_data['user_id']==user_id]['product_id'].nunique()} products\")\n",
    "    print(f\"  Top 5 recommendations:\")\n",
    "    \n",
    "    for i, (prod_id, score) in enumerate(recs, 1):\n",
    "        prod_name = products[products['product_id']==prod_id]['product_name'].values[0]\n",
    "        print(f\"    {i}. {prod_name} (score: {score:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Section 2.1 Complete - CF Models Trained\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c2719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2.2 - Content-Based Filtering\n",
      "============================================================\n",
      "\n",
      "Building Item Profiles...\n",
      " Products with metadata: 49,688\n",
      "   Departments: 21\n",
      "   Aisles: 134\n",
      "\n",
      "Item Profiles:\n",
      "  Products: 49,688\n",
      "  Features: 155\n",
      "\n",
      "------------------------------------------------------------\n",
      "Creating User Profiles\n",
      "------------------------------------------------------------\n",
      " User profile function created\n",
      " CBF recommendation function created\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing Content-Based Recommendations\n",
      "------------------------------------------------------------\n",
      "\n",
      "CBF Recommendations:\n",
      "----------------------------------------\n",
      "\n",
      "User 132637:\n",
      "  Past purchases: 28 products\n",
      "  Top 5 recommendations:\n",
      "    1. Organic Lemons (score: 0.691)\n",
      "    2. Nectarines (score: 0.691)\n",
      "    3. Cantaloupe (score: 0.691)\n",
      "    4. Red Seedless Grapes Imported (score: 0.691)\n",
      "    5. Mini Watermelon (score: 0.691)\n",
      "\n",
      "User 160970:\n",
      "  Past purchases: 52 products\n",
      "  Top 5 recommendations:\n",
      "    1. Organic Lemons (score: 0.655)\n",
      "    2. Nectarines (score: 0.655)\n",
      "    3. Cantaloupe (score: 0.655)\n",
      "    4. Red Seedless Grapes Imported (score: 0.655)\n",
      "    5. Mini Watermelon (score: 0.655)\n",
      "\n",
      "User 151132:\n",
      "  Past purchases: 80 products\n",
      "  Top 5 recommendations:\n",
      "    1. Banana & Sweet Potato Organic Teething Wafers (score: 0.615)\n",
      "    2. Autumn Vegetable & Turkey Dinner with Lil' Bits Purees Dinner (score: 0.615)\n",
      "    3. Organic Blueberry Blitz Fruit & Veggie Smoothie Mashups (score: 0.615)\n",
      "    4. Organic Yummy Tummy Maple & Brown Sugar Instant Oatmeal (score: 0.615)\n",
      "    5. Graduates Lil' Entrees Spiral Pasta In Turkey Meat Sauce With Green & Yellow Beans (score: 0.615)\n",
      "\n",
      "============================================================\n",
      "Section 2.2 Complete - CBF with User Profiles\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 2.2 - Content-Based Filtering (CBF)\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2.2 - Content-Based Filtering\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Build Item Profiles\n",
    "# -----------------------------------------------\n",
    "print(\"Building Item Profiles...\")\n",
    "\n",
    "# Merge product metadata\n",
    "products_full = products.merge(departments, on='department_id', how='left')\n",
    "products_full = products_full.merge(aisles, on='aisle_id', how='left')\n",
    "\n",
    "print(f\" Products with metadata: {len(products_full):,}\")\n",
    "print(f\"   Departments: {products_full['department_id'].nunique()}\")\n",
    "print(f\"   Aisles: {products_full['aisle_id'].nunique()}\")\n",
    "\n",
    "# Create feature vectors\n",
    "# One-hot encode departments and aisles\n",
    "dept_encoded = pd.get_dummies(products_full['department_id'], prefix='dept')\n",
    "aisle_encoded = pd.get_dummies(products_full['aisle_id'], prefix='aisle')\n",
    "\n",
    "# Combine features\n",
    "item_profile = pd.concat([\n",
    "    products_full[['product_id']],\n",
    "    dept_encoded,\n",
    "    aisle_encoded\n",
    "], axis=1)\n",
    "\n",
    "print(f\"\\nItem Profiles:\")\n",
    "print(f\"  Products: {len(item_profile):,}\")\n",
    "print(f\"  Features: {item_profile.shape[1] - 1}\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Create User Profiles (Weighted Item Features)\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Creating User Profiles\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "def create_user_profile(user_id, purchase_counts, item_features):\n",
    "    \"\"\"\n",
    "    Create weighted user profile based on purchase history\n",
    "    \n",
    "    Args:\n",
    "        user_id: Target user ID\n",
    "        purchase_counts: DataFrame with [user_id, product_id, frequency]\n",
    "        item_features: Items profiles (product_id + features)\n",
    "        \n",
    "    Returns:\n",
    "        User profile vector (weighted average of item features)\n",
    "    \"\"\"\n",
    "    # Get user's purchases\n",
    "    user_purchases = purchase_counts[purchase_counts['user_id'] == user_id]\n",
    "    \n",
    "    if len(user_purchases) == 0:\n",
    "        return None\n",
    "    \n",
    "    purchased_items = user_purchases['product_id'].values\n",
    "    purchase_freqs = user_purchases['frequency'].values\n",
    "    \n",
    "    # Apply SAME log transformation as CF\n",
    "    log_freqs = np.log1p(purchase_freqs)\n",
    "    \n",
    "    # Normalize to weights (sum to 1)\n",
    "    weights = log_freqs / log_freqs.sum()\n",
    "    \n",
    "    # Get item feature vectors\n",
    "    item_vectors = item_features.loc[\n",
    "        item_features['product_id'].isin(purchased_items)\n",
    "    ].drop('product_id', axis=1).values\n",
    "    \n",
    "    # Weighted average of item features\n",
    "    user_profile = (item_vectors.T @ weights).reshape(1, -1)\n",
    "    \n",
    "    return user_profile\n",
    "\n",
    "print(f\" User profile function created\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# CBF Recommendation Function\n",
    "# -----------------------------------------------\n",
    "\n",
    "def get_cbf_recommendations(user_id, n=10):\n",
    "    \"\"\"\n",
    "    Get content-based recommendations using user profile\n",
    "    \n",
    "    Args:\n",
    "        user_id: Target user ID\n",
    "        n: Number of recommendations\n",
    "        \n",
    "    Returns:\n",
    "        List of (product_id, score) tuples\n",
    "    \"\"\"\n",
    "    # Create user profile\n",
    "    user_profile = create_user_profile(user_id, purchase_counts, item_profile)\n",
    "    \n",
    "    if user_profile is None:\n",
    "        return []\n",
    "    \n",
    "    # Get user's already purchased items\n",
    "    user_products = purchase_counts[purchase_counts['user_id'] == user_id]['product_id'].values\n",
    "    \n",
    "    # Compute similarity between user profile and all items\n",
    "    feature_matrix = item_profile.drop('product_id', axis=1).values\n",
    "    similarities = cosine_similarity(user_profile, feature_matrix)[0]\n",
    "    \n",
    "    # Create product_id to index mapping\n",
    "    idx_to_product = {idx: pid for idx, pid in enumerate(item_profile['product_id'])}\n",
    "\n",
    "    # Get top N similar items (excluding already purchased)\n",
    "    recommendations = []\n",
    "    for idx, score in enumerate(similarities):\n",
    "        product_id = idx_to_product[idx]\n",
    "        if product_id not in user_products:\n",
    "            recommendations.append((product_id, score))\n",
    "    \n",
    "    # Sort by score and return top N\n",
    "    recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "    return recommendations[:n]\n",
    "\n",
    "print(\" CBF recommendation function created\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Test CBF Recommendations\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Testing Content-Based Recommendations\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Test on same sample users as CF\n",
    "print(\"\\nCBF Recommendations:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for user_id in sample_users:\n",
    "    recs = get_cbf_recommendations(user_id, n=5)\n",
    "    \n",
    "    print(f\"\\nUser {user_id}:\")\n",
    "    print(f\"  Past purchases: {purchase_counts[purchase_counts['user_id']==user_id]['product_id'].nunique()} products\")\n",
    "    print(f\"  Top 5 recommendations:\")\n",
    "    \n",
    "    if len(recs) == 0:\n",
    "        print(\"    (No recommendations - user not in training data)\")\n",
    "    else:\n",
    "        for i, (prod_id, score) in enumerate(recs, 1):\n",
    "            prod_name = products[products['product_id']==prod_id]['product_name'].values[0]\n",
    "            print(f\"    {i}. {prod_name} (score: {score:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Section 2.2 Complete - CBF with User Profiles\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8a5be094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2.3 - Hybrid Recommendations\n",
      "============================================================\n",
      "\n",
      "Creating hybrid recommendation system...\n",
      " Hybrid recommendation function created\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing Hybrid Recommendations with Different Weights\n",
      "------------------------------------------------------------\n",
      "\n",
      "User 132637:\n",
      "Past purchases: 28 products\n",
      "\n",
      "CF-Heavy (CF=0.7, CBF=0.3):\n",
      "----------------------------------------\n",
      "  1. Half And Half Ultra Pasteurized (score: 0.700)\n",
      "  2. Organic Lactose Free Whole Milk (score: 0.434)\n",
      "  3. Bananas (score: 0.416)\n",
      "  4. 1% Milkfat Low Fat Vitamin A & D Milk (score: 0.409)\n",
      "  5. Organic Reduced Fat Milk (score: 0.397)\n",
      "\n",
      "Balanced (CF=0.5, CBF=0.5):\n",
      "----------------------------------------\n",
      "  1. Bananas (score: 0.583)\n",
      "  2. Bag of Organic Bananas (score: 0.521)\n",
      "  3. Ataulfo Mango (score: 0.500)\n",
      "  4. Organic Red Delicious Apples (score: 0.500)\n",
      "  5. Mandarin Clementine  Bag (score: 0.500)\n",
      "\n",
      "CBF-Heavy (CF=0.3, CBF=0.7):\n",
      "----------------------------------------\n",
      "  1. Bananas (score: 0.750)\n",
      "  2. Bag of Organic Bananas (score: 0.713)\n",
      "  3. Ataulfo Mango (score: 0.700)\n",
      "  4. Organic Red Delicious Apples (score: 0.700)\n",
      "  5. Mandarin Clementine  Bag (score: 0.700)\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Side-by-Side Comparison: CF vs CBF vs Hybrid\n",
      "------------------------------------------------------------\n",
      "\n",
      "User 160970:\n",
      "Past purchases: 52 products\n",
      "\n",
      "CF Only                                            | CBF Only                                           | Hybrid (50/50)                                    \n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "1. Organic Low Fat Milk                            | 1. Organic Lemons                                  | 1. Banana                                         \n",
      "2. Goat Milk                                       | 2. Nectarines                                      | 2. Bananas                                        \n",
      "3. Organic Whole Milk with DHA Omega-3             | 3. Cantaloupe                                      | 3. Organic Large Extra Fancy Fuji Apple           \n",
      "4. Organic Homogenized Whole Milk                  | 4. Red Seedless Grapes Imported                    | 4. Organic Hass Avocado                           \n",
      "5. Organic Grade A Raw Whole Milk                  | 5. Mini Watermelon                                 | 5. Honeycrisp Apple                               \n",
      "\n",
      "============================================================\n",
      "Section 2.3 Complete - Hybrid System Built\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 2.3 - Hybrid Recommendations\n",
    "# ================================================================\n",
    "\n",
    "# Normalization Strategy:\n",
    "# - Retrieve top-400 candidates from both CF and CBF to ensure \n",
    "#   sufficient score variance (CBF assigns many tied scores due to \n",
    "#   limited product features). Normalized scores then combined via\n",
    "#   weighted average.\n",
    "# - Final recommendations: top-5 shown here; evaluation at k={5,10,20}\n",
    "#   performed in Section 2.4\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2.3 - Hybrid Recommendations\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Hybrid Recommendation Function\n",
    "# -----------------------------------------------\n",
    "print(\"Creating hybrid recommendation system...\")\n",
    "\n",
    "def get_hybrid_recommendations(user_id, n=10, cf_weight=0.5, cbf_weight=0.5, debug=False):\n",
    "    \"\"\"\n",
    "    Combine CF and CBF recommendations with weighted scoring\n",
    "    \n",
    "    Args:\n",
    "        user_id: Target user ID\n",
    "        n: Number of recommendations\n",
    "        cf_weight: Weight for collaborative filtering (0-1)\n",
    "        cbf_weight: Weight for content-based filtering (0-1)\n",
    "        debug: Print diagnostic information\n",
    "        \n",
    "    Returns:\n",
    "        List of (product_id, combined_score) tuples\n",
    "    \"\"\"\n",
    "    # Get broader candidate set for better normalization (top-400)\n",
    "    cf_recs = get_top_n_recommendations(svd_model, user_id, n=400, exclude_purchased=True)\n",
    "    cbf_recs = get_cbf_recommendations(user_id, n=400)\n",
    "    \n",
    "    # Debug: Show raw score distributions\n",
    "    if debug:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DIAGNOSTIC for User {user_id}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if len(cf_recs) > 0:\n",
    "            cf_raw = [score for _, score in cf_recs]\n",
    "            print(f\"\\nCF Raw Scores (top {len(cf_recs)}):\")\n",
    "            print(f\"  Min: {min(cf_raw):.4f}, Max: {max(cf_raw):.4f}\")\n",
    "            print(f\"  Mean: {np.mean(cf_raw):.4f}, Std: {np.std(cf_raw):.4f}\")\n",
    "            print(f\"  Range: {max(cf_raw) - min(cf_raw):.4f}\")\n",
    "        \n",
    "        if len(cbf_recs) > 0:\n",
    "            cbf_raw = [score for _, score in cbf_recs]\n",
    "            print(f\"\\nCBF Raw Scores (top {len(cbf_recs)}):\")\n",
    "            print(f\"  Min: {min(cbf_raw):.4f}, Max: {max(cbf_raw):.4f}\")\n",
    "            print(f\"  Mean: {np.mean(cbf_raw):.4f}, Std: {np.std(cbf_raw):.4f}\")\n",
    "            print(f\"  Range: {max(cbf_raw) - min(cbf_raw):.4f}\")\n",
    "    \n",
    "    # Normalize scores to 0-1 range for fair combination\n",
    "    # CF scores\n",
    "    if len(cf_recs) > 0:\n",
    "        cf_scores_dict = {pid: score for pid, score in cf_recs}\n",
    "        cf_min = min(cf_scores_dict.values())\n",
    "        cf_max = max(cf_scores_dict.values())\n",
    "        cf_range = cf_max - cf_min if cf_max > cf_min else 1\n",
    "        cf_scores_norm = {pid: (score - cf_min) / cf_range for pid, score in cf_scores_dict.items()}\n",
    "    else:\n",
    "        cf_scores_norm = {}\n",
    "    \n",
    "    # CBF scores\n",
    "    if len(cbf_recs) > 0:\n",
    "        cbf_scores_dict = {pid: score for pid, score in cbf_recs}\n",
    "        cbf_min = min(cbf_scores_dict.values())\n",
    "        cbf_max = max(cbf_scores_dict.values())\n",
    "        cbf_range = cbf_max - cbf_min if cbf_max > cbf_min else 1\n",
    "        cbf_scores_norm = {pid: (score - cbf_min) / cbf_range for pid, score in cbf_scores_dict.items()}\n",
    "    else:\n",
    "        cbf_scores_norm = {}\n",
    "    \n",
    "    # Debug: Show normalized distributions\n",
    "    if debug:\n",
    "        print(f\"\\nAfter Min-Max Normalization:\")\n",
    "        if len(cf_scores_norm) > 0:\n",
    "            cf_norm_vals = list(cf_scores_norm.values())\n",
    "            print(f\"CF Normalized: Min={min(cf_norm_vals):.4f}, Max={max(cf_norm_vals):.4f}, Mean={np.mean(cf_norm_vals):.4f}\")\n",
    "        if len(cbf_scores_norm) > 0:\n",
    "            cbf_norm_vals = list(cbf_scores_norm.values())\n",
    "            print(f\"CBF Normalized: Min={min(cbf_norm_vals):.4f}, Max={max(cbf_norm_vals):.4f}, Mean={np.mean(cbf_norm_vals):.4f}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Combine scores\n",
    "    all_products = set(cf_scores_norm.keys()) | set(cbf_scores_norm.keys())\n",
    "    \n",
    "    hybrid_scores = {}\n",
    "    for product_id in all_products:\n",
    "        cf_score = cf_scores_norm.get(product_id, 0)\n",
    "        cbf_score = cbf_scores_norm.get(product_id, 0)\n",
    "        \n",
    "        # Weighted combination\n",
    "        hybrid_scores[product_id] = (cf_weight * cf_score) + (cbf_weight * cbf_score)\n",
    "    \n",
    "    # Sort by combined score and return top n\n",
    "    recommendations = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "print(\" Hybrid recommendation function created\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Test Different Weight Combinations\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Testing Hybrid Recommendations with Different Weights\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Test user\n",
    "test_user = sample_users[0]\n",
    "\n",
    "print(f\"\\nUser {test_user}:\")\n",
    "print(f\"Past purchases: {cf_data[cf_data['user_id']==test_user]['product_id'].nunique()} products\\n\")\n",
    "\n",
    "# Test different weight combinations\n",
    "weight_configs = [\n",
    "    (0.7, 0.3, \"CF-Heavy\"),\n",
    "    (0.5, 0.5, \"Balanced\"),\n",
    "    (0.3, 0.7, \"CBF-Heavy\")\n",
    "]\n",
    "\n",
    "for cf_w, cbf_w, label in weight_configs:\n",
    "    print(f\"{label} (CF={cf_w}, CBF={cbf_w}):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    recs = get_hybrid_recommendations(test_user, n=5, cf_weight=cf_w, cbf_weight=cbf_w)\n",
    "    \n",
    "    for i, (prod_id, score) in enumerate(recs, 1):\n",
    "        prod_name = products[products['product_id']==prod_id]['product_name'].values[0]\n",
    "        print(f\"  {i}. {prod_name} (score: {score:.3f})\")\n",
    "    print()\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Compare All Three Approaches\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Side-by-Side Comparison: CF vs CBF vs Hybrid\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "comparison_user = sample_users[1]\n",
    "\n",
    "print(f\"\\nUser {comparison_user}:\")\n",
    "print(f\"Past purchases: {cf_data[cf_data['user_id']==comparison_user]['product_id'].nunique()} products\\n\")\n",
    "\n",
    "# Get recommendations from all methods\n",
    "cf_only = get_top_n_recommendations(svd_model, comparison_user, n=5)\n",
    "cbf_only = get_cbf_recommendations(comparison_user, n=5)\n",
    "hybrid = get_hybrid_recommendations(comparison_user, n=5, cf_weight=0.5, cbf_weight=0.5)\n",
    "\n",
    "# Display side by side\n",
    "print(f\"{'CF Only':<50} | {'CBF Only':<50} | {'Hybrid (50/50)':<50}\")\n",
    "print(\"-\" * 155)\n",
    "\n",
    "for i in range(5):\n",
    "    # CF\n",
    "    if i < len(cf_only):\n",
    "        cf_name = products[products['product_id']==cf_only[i][0]]['product_name'].values[0]\n",
    "        cf_text = f\"{i+1}. {cf_name[:40]}\"\n",
    "    else:\n",
    "        cf_text = \"\"\n",
    "    \n",
    "    # CBF\n",
    "    if i < len(cbf_only):\n",
    "        cbf_name = products[products['product_id']==cbf_only[i][0]]['product_name'].values[0]\n",
    "        cbf_text = f\"{i+1}. {cbf_name[:40]}\"\n",
    "    else:\n",
    "        cbf_text = \"\"\n",
    "    \n",
    "    # Hybrid\n",
    "    if i < len(hybrid):\n",
    "        hyb_name = products[products['product_id']==hybrid[i][0]]['product_name'].values[0]\n",
    "        hyb_text = f\"{i+1}. {hyb_name[:40]}\"\n",
    "    else:\n",
    "        hyb_text = \"\"\n",
    "    \n",
    "    print(f\"{cf_text:<50} | {cbf_text:<50} | {hyb_text:<50}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Section 2.3 Complete - Hybrid System Built\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d402a7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2.4 - Evaluation of Global Models\n",
      "============================================================\n",
      "\n",
      "Preparing validation ground truth...\n",
      " Total validation users: 175,072\n",
      " Total validation purchases: 1,830,111\n",
      "\n",
      "Performing stratified sampling...\n",
      " Sampled 2,000 users for evaluation\n",
      "  Users per segment: ~400\n",
      "  Segment distribution:\n",
      "    Segment 0 (Power Users): 400 users\n",
      "    Segment 1 (Routine Snackers): 400 users\n",
      "    Segment 2 (Bulk Shoppers): 400 users\n",
      "    Segment 3 (Alcohol Enthusiasts): 400 users\n",
      "    Segment 4 (Household Essentials): 400 users\n",
      "\n",
      "Caching user purchase histories for faster processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building cache: 100%|██████████| 2000/2000 [00:42<00:00, 47.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cached purchase histories for 2,000 users\n",
      "\n",
      "------------------------------------------------------------\n",
      "Creating Baseline Model (Popularity-Based)\n",
      "------------------------------------------------------------\n",
      " Most popular products computed from training data\n",
      " Baseline recommendation function created\n",
      "\n",
      "------------------------------------------------------------\n",
      "Defining Evaluation Metrics\n",
      "------------------------------------------------------------\n",
      " Evaluation metrics defined: Precision@K, Recall@K, F1@K\n",
      "\n",
      "------------------------------------------------------------\n",
      "Profiling Performance on First 20 Users\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Profiling: 100%|██████████| 20/20 [00:26<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timing Summary (average per user):\n",
      "----------------------------------------\n",
      "  BASELINE    : 0.002s\n",
      "  CF          : 0.406s\n",
      "  CBF         : 0.261s\n",
      "  HYBRID      : 0.669s\n",
      "\n",
      "  Total per user: 1.339s\n",
      "  Estimated time for 2,000 users: 44.6 minutes\n",
      "\n",
      "------------------------------------------------------------\n",
      "Evaluating Models on Full Validation Sample\n",
      "------------------------------------------------------------\n",
      "Evaluating on 2,000 sampled validation users...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing users: 100%|██████████| 2000/2000 [2:09:33<00:00,  3.89s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation complete\n",
      "\n",
      "------------------------------------------------------------\n",
      "Global Model Performance (Validation Set)\n",
      "------------------------------------------------------------\n",
      "\n",
      "   Model    P@5      R@5     F1@5    P@10     R@10    F1@10     P@20     R@20    F1@20\n",
      "Baseline 0.0126 0.007857 0.008421 0.01055 0.014319 0.010418 0.007800 0.018921 0.009896\n",
      "      CF 0.0003 0.000141 0.000192 0.00045 0.000496 0.000450 0.000475 0.001133 0.000624\n",
      "     CBF 0.0006 0.000154 0.000234 0.00055 0.000419 0.000426 0.000600 0.001073 0.000692\n",
      "  Hybrid 0.0017 0.001512 0.001304 0.00125 0.002029 0.001342 0.000975 0.002888 0.001319\n",
      "\n",
      " Best performing model: Baseline\n",
      "  (Selected based on average F1 score across K ∈ {5, 10, 20})\n",
      "\n",
      "============================================================\n",
      "Section 2.4 Complete - Global Models Evaluated\n",
      "============================================================\n",
      "\n",
      "Note: Results based on stratified sample of 2,000 users\n",
      "      (computational constraints, representative across all segments)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 2.4 - Evaluation of Global Models\n",
    "# ================================================================\n",
    "# \n",
    "# Goal: Compare 4 recommendation approaches on validation set:\n",
    "#   1. Baseline (popularity-based)\n",
    "#   2. Collaborative Filtering (SVD)\n",
    "#   3. Content-Based Filtering (cosine similarity)\n",
    "#   4. Hybrid (CF + CBF weighted combination)\n",
    "#\n",
    "# Metrics: Precision@K, Recall@K, F1@K for K ∈ {5, 10, 20}\n",
    "# Best model selected for segment-specific training in Section 2.5\n",
    "#\n",
    "# Note: Due to computational constraints, evaluation performed on \n",
    "# stratified sample of 2,000 validation users (400 per segment)\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2.4 - Evaluation of Global Models\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Prepare Validation Ground Truth\n",
    "# -----------------------------------------------\n",
    "print(\"Preparing validation ground truth...\")\n",
    "\n",
    "# Merge orders with order_products to get user_id → product_id mapping\n",
    "val_data = orders_val[['order_id', 'user_id', 'cluster']].merge(\n",
    "    order_products_val[['order_id', 'product_id']], \n",
    "    on='order_id'\n",
    ")\n",
    "\n",
    "# Get validation ground truth (actual purchases per user)\n",
    "val_ground_truth = val_data.groupby('user_id')['product_id'].apply(list).to_dict()\n",
    "\n",
    "print(f\" Total validation users: {len(val_ground_truth):,}\")\n",
    "print(f\" Total validation purchases: {len(val_data):,}\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Stratified Sampling for Computational Efficiency\n",
    "# -----------------------------------------------\n",
    "print(\"\\nPerforming stratified sampling...\")\n",
    "\n",
    "SAMPLE_SIZE = 2000\n",
    "USERS_PER_SEGMENT = SAMPLE_SIZE // 5\n",
    "\n",
    "# Sample equal number of users from each segment\n",
    "val_users_df = orders_val[['user_id', 'cluster']].drop_duplicates()\n",
    "val_users_sampled = (\n",
    "    val_users_df.groupby('cluster', group_keys=False)\n",
    "    .apply(lambda x: x.sample(min(len(x), USERS_PER_SEGMENT), random_state=SEED))\n",
    ")\n",
    "\n",
    "# Filter to users with ground truth\n",
    "val_users = [u for u in val_users_sampled['user_id'].values if u in val_ground_truth]\n",
    "\n",
    "print(f\" Sampled {len(val_users):,} users for evaluation\")\n",
    "print(f\"  Users per segment: ~{USERS_PER_SEGMENT}\")\n",
    "print(f\"  Segment distribution:\")\n",
    "for cluster_id in range(5):\n",
    "    count = sum(val_users_df[val_users_df['user_id'].isin(val_users)]['cluster'] == cluster_id)\n",
    "    segment_name = user_features_clustered[user_features_clustered['cluster']==cluster_id]['segment_name'].iloc[0]\n",
    "    print(f\"    Segment {cluster_id} ({segment_name}): {count} users\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Cache User Purchase Histories\n",
    "# -----------------------------------------------\n",
    "print(\"\\nCaching user purchase histories for faster processing...\")\n",
    "\n",
    "user_purchase_cache = {}\n",
    "for user_id in tqdm(val_users, desc=\"Building cache\"):\n",
    "    user_purchase_cache[user_id] = set(train_interactions[train_interactions['user_id']==user_id]['product_id'])\n",
    "\n",
    "print(f\" Cached purchase histories for {len(user_purchase_cache):,} users\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Baseline: Popularity-Based Recommendations\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Creating Baseline Model (Popularity-Based)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Calculate global popularity from training data\n",
    "global_popularity = (train_interactions\n",
    "                     .groupby('product_id')\n",
    "                     .size()\n",
    "                     .reset_index(name='frequency')\n",
    "                     .sort_values('frequency', ascending=False))\n",
    "\n",
    "print(f\" Most popular products computed from training data\")\n",
    "\n",
    "def get_baseline_recommendations(user_id, n=10):\n",
    "    \"\"\"\n",
    "    Baseline: Recommend most popular products globally\n",
    "    (excluding user's past purchases)\n",
    "    \n",
    "    Args:\n",
    "        user_id: Target user ID\n",
    "        n: Number of recommendations\n",
    "        \n",
    "    Returns:\n",
    "        List of product_ids\n",
    "    \"\"\"\n",
    "    # Get user's purchase history from cache\n",
    "    user_purchases = user_purchase_cache.get(user_id, set())\n",
    "    \n",
    "    # Filter out already purchased items\n",
    "    recommendations = global_popularity[~global_popularity['product_id'].isin(user_purchases)]\n",
    "    \n",
    "    return list(recommendations['product_id'].head(n))\n",
    "\n",
    "print(\" Baseline recommendation function created\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Evaluation Metrics\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Defining Evaluation Metrics\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "def precision_at_k(recommended, actual, k):\n",
    "    \"\"\"Precision@K: Proportion of recommended items that were purchased\"\"\"\n",
    "    rec_k = recommended[:k]\n",
    "    relevant = set(rec_k) & set(actual)\n",
    "    return len(relevant) / k if k > 0 else 0\n",
    "\n",
    "def recall_at_k(recommended, actual, k):\n",
    "    \"\"\"Recall@K: Proportion of purchased items that were recommended\"\"\"\n",
    "    rec_k = recommended[:k]\n",
    "    relevant = set(rec_k) & set(actual)\n",
    "    return len(relevant) / len(actual) if len(actual) > 0 else 0\n",
    "\n",
    "def f1_at_k(recommended, actual, k):\n",
    "    \"\"\"F1@K: Harmonic mean of Precision@K and Recall@K\"\"\"\n",
    "    prec = precision_at_k(recommended, actual, k)\n",
    "    rec = recall_at_k(recommended, actual, k)\n",
    "    return 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0\n",
    "\n",
    "print(\" Evaluation metrics defined: Precision@K, Recall@K, F1@K\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Performance Profiling (First 20 Users)\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Profiling Performance on First 20 Users\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "import time\n",
    "\n",
    "profile_users = val_users[:20]\n",
    "timing_results = {\n",
    "    'baseline': [],\n",
    "    'cf': [],\n",
    "    'cbf': [],\n",
    "    'hybrid': []\n",
    "}\n",
    "\n",
    "for user_id in tqdm(profile_users, desc=\"Profiling\"):\n",
    "    actual = val_ground_truth[user_id]\n",
    "    \n",
    "    # Time each model\n",
    "    start = time.time()\n",
    "    baseline_recs = get_baseline_recommendations(user_id, n=20)\n",
    "    timing_results['baseline'].append(time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    cf_recs = [pid for pid, _ in get_top_n_recommendations(svd_model, user_id, n=20, exclude_purchased=True)]\n",
    "    timing_results['cf'].append(time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    cbf_recs = [pid for pid, _ in get_cbf_recommendations(user_id, n=20)]\n",
    "    timing_results['cbf'].append(time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    hybrid_recs = [pid for pid, _ in get_hybrid_recommendations(user_id, n=20, cf_weight=0.5, cbf_weight=0.5)]\n",
    "    timing_results['hybrid'].append(time.time() - start)\n",
    "\n",
    "# Display timing results\n",
    "print(\"\\nTiming Summary (average per user):\")\n",
    "print(\"-\" * 40)\n",
    "for model_name, times in timing_results.items():\n",
    "    avg_time = np.mean(times)\n",
    "    print(f\"  {model_name.upper():12s}: {avg_time:.3f}s\")\n",
    "\n",
    "total_avg = sum(np.mean(times) for times in timing_results.values())\n",
    "print(f\"\\n  Total per user: {total_avg:.3f}s\")\n",
    "print(f\"  Estimated time for {len(val_users):,} users: {(total_avg * len(val_users) / 60):.1f} minutes\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Evaluate All Models on Validation Set\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Evaluating Models on Full Validation Sample\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Initialize results storage\n",
    "results = {\n",
    "    'Baseline': {'P@5': [], 'R@5': [], 'F1@5': [], \n",
    "                 'P@10': [], 'R@10': [], 'F1@10': [],\n",
    "                 'P@20': [], 'R@20': [], 'F1@20': []},\n",
    "    'CF': {'P@5': [], 'R@5': [], 'F1@5': [], \n",
    "           'P@10': [], 'R@10': [], 'F1@10': [],\n",
    "           'P@20': [], 'R@20': [], 'F1@20': []},\n",
    "    'CBF': {'P@5': [], 'R@5': [], 'F1@5': [], \n",
    "            'P@10': [], 'R@10': [], 'F1@10': [],\n",
    "            'P@20': [], 'R@20': [], 'F1@20': []},\n",
    "    'Hybrid': {'P@5': [], 'R@5': [], 'F1@5': [], \n",
    "               'P@10': [], 'R@10': [], 'F1@10': [],\n",
    "               'P@20': [], 'R@20': [], 'F1@20': []}\n",
    "}\n",
    "\n",
    "print(f\"Evaluating on {len(val_users):,} sampled validation users...\\n\")\n",
    "\n",
    "for user_id in tqdm(val_users, desc=\"Processing users\"):\n",
    "    actual = val_ground_truth[user_id]\n",
    "    \n",
    "    # Generate recommendations from all models\n",
    "    baseline_recs = get_baseline_recommendations(user_id, n=20)\n",
    "    cf_recs = [pid for pid, _ in get_top_n_recommendations(svd_model, user_id, n=20, exclude_purchased=True)]\n",
    "    cbf_recs = [pid for pid, _ in get_cbf_recommendations(user_id, n=20)]\n",
    "    hybrid_recs = [pid for pid, _ in get_hybrid_recommendations(user_id, n=20, cf_weight=0.5, cbf_weight=0.5)]\n",
    "    \n",
    "    # Evaluate at K = 5, 10, 20\n",
    "    for k in [5, 10, 20]:\n",
    "        for model_name, recs in [('Baseline', baseline_recs), \n",
    "                                  ('CF', cf_recs), \n",
    "                                  ('CBF', cbf_recs), \n",
    "                                  ('Hybrid', hybrid_recs)]:\n",
    "            results[model_name][f'P@{k}'].append(precision_at_k(recs, actual, k))\n",
    "            results[model_name][f'R@{k}'].append(recall_at_k(recs, actual, k))\n",
    "            results[model_name][f'F1@{k}'].append(f1_at_k(recs, actual, k))\n",
    "\n",
    "print(\" Evaluation complete\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Aggregate Results\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Global Model Performance (Validation Set)\")\n",
    "print(\"-\"*60 + \"\\n\")\n",
    "\n",
    "# Compute mean metrics across sampled users\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': [],\n",
    "    'P@5': [], 'R@5': [], 'F1@5': [],\n",
    "    'P@10': [], 'R@10': [], 'F1@10': [],\n",
    "    'P@20': [], 'R@20': [], 'F1@20': []\n",
    "})\n",
    "\n",
    "for model_name in ['Baseline', 'CF', 'CBF', 'Hybrid']:\n",
    "    row = {'Model': model_name}\n",
    "    for metric in ['P@5', 'R@5', 'F1@5', 'P@10', 'R@10', 'F1@10', 'P@20', 'R@20', 'F1@20']:\n",
    "        # Average metric across sampled validation users\n",
    "        row[metric] = np.mean(results[model_name][metric])\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Display results\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Identify best model based on average F1 score\n",
    "best_f1_avg = results_df[['F1@5', 'F1@10', 'F1@20']].mean(axis=1)\n",
    "best_model = results_df.loc[best_f1_avg.idxmax(), 'Model']\n",
    "\n",
    "print(f\"\\n Best performing model: {best_model}\")\n",
    "print(f\"  (Selected based on average F1 score across K ∈ {{5, 10, 20}})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Section 2.4 Complete - Global Models Evaluated\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nNote: Results based on stratified sample of {len(val_users):,} users\")\n",
    "print(\"      (computational constraints, representative across all segments)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a802aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2.5 - Train Segment-Specific Models\n",
      "============================================================\n",
      "\n",
      "Best global model from Section 2.4: Baseline\n",
      "Training segment-specific models for all approaches...\n",
      "\n",
      "------------------------------------------------------------\n",
      "Caching Training User Purchase Histories\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building training cache: 100%|██████████| 175072/175072 [1:15:07<00:00, 38.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cached purchase histories for 175,072 training users\n",
      "\n",
      "------------------------------------------------------------\n",
      "1. Creating Segment-Specific Baseline Models\n",
      "------------------------------------------------------------\n",
      "\n",
      "Segment 0: Power Users\n",
      "  Users: 78,862\n",
      "  Interactions: 18,402,176\n",
      "  Unique products: 43,969\n",
      "  Top 3 popular products:\n",
      "    - Banana (300299 purchases)\n",
      "    - Bag of Organic Bananas (274348 purchases)\n",
      "    - Organic Strawberries (205765 purchases)\n",
      "\n",
      "Segment 1: Routine Snackers\n",
      "  Users: 23,533\n",
      "  Interactions: 2,579,429\n",
      "  Unique products: 36,598\n",
      "  Top 3 popular products:\n",
      "    - Banana (31337 purchases)\n",
      "    - Bag of Organic Bananas (26537 purchases)\n",
      "    - Soda (22661 purchases)\n",
      "\n",
      "Segment 2: Bulk Shoppers\n",
      "  Users: 63,000\n",
      "  Interactions: 7,310,155\n",
      "  Unique products: 48,046\n",
      "  Top 3 popular products:\n",
      "    - Banana (88853 purchases)\n",
      "    - Bag of Organic Bananas (38467 purchases)\n",
      "    - Strawberries (27246 purchases)\n",
      "\n",
      "Segment 3: Alcohol Enthusiasts\n",
      "  Users: 1,906\n",
      "  Interactions: 129,278\n",
      "  Unique products: 11,355\n",
      "  Top 3 popular products:\n",
      "    - Sauvignon Blanc (2492 purchases)\n",
      "    - India Pale Ale (2221 purchases)\n",
      "    - Beer (2193 purchases)\n",
      "\n",
      "Segment 4: Household Essentials\n",
      "  Users: 7,771\n",
      "  Interactions: 593,452\n",
      "  Unique products: 30,227\n",
      "  Top 3 popular products:\n",
      "    - Banana (4507 purchases)\n",
      "    - Bag of Organic Bananas (2551 purchases)\n",
      "    - Strawberries (1793 purchases)\n",
      "\n",
      " All 5 segment-specific baseline models created\n",
      "\n",
      "------------------------------------------------------------\n",
      "2. Training Segment-Specific CF Models\n",
      "------------------------------------------------------------\n",
      "\n",
      "Training CF model for Segment 0: Power Users\n",
      "----------------------------------------\n",
      "  Users: 78,862\n",
      "  Interactions: 6,502,922\n",
      "  Rating statistics:\n",
      "    Min: 0.693\n",
      "    Max: 4.595\n",
      "    Mean: 1.099\n",
      "  Model trained\n",
      "\n",
      "Training CF model for Segment 1: Routine Snackers\n",
      "----------------------------------------\n",
      "  Users: 23,533\n",
      "  Interactions: 904,801\n",
      "  Rating statistics:\n",
      "    Min: 0.693\n",
      "    Max: 4.585\n",
      "    Mean: 1.093\n",
      "  Model trained\n",
      "\n",
      "Training CF model for Segment 2: Bulk Shoppers\n",
      "----------------------------------------\n",
      "  Users: 63,000\n",
      "  Interactions: 3,843,609\n",
      "  Rating statistics:\n",
      "    Min: 0.693\n",
      "    Max: 4.543\n",
      "    Mean: 0.940\n",
      "  Model trained\n",
      "\n",
      "Training CF model for Segment 3: Alcohol Enthusiasts\n",
      "----------------------------------------\n",
      "  Users: 1,906\n",
      "  Interactions: 63,278\n",
      "  Rating statistics:\n",
      "    Min: 0.693\n",
      "    Max: 4.585\n",
      "    Mean: 0.947\n",
      "  Model trained\n",
      "\n",
      "Training CF model for Segment 4: Household Essentials\n",
      "----------------------------------------\n",
      "  Users: 7,771\n",
      "  Interactions: 314,694\n",
      "  Rating statistics:\n",
      "    Min: 0.693\n",
      "    Max: 4.522\n",
      "    Mean: 0.927\n",
      "  Model trained\n",
      "\n",
      " All 5 segment-specific CF models trained\n",
      "\n",
      "------------------------------------------------------------\n",
      "3. Creating Segment-Specific Recommendation Functions\n",
      "------------------------------------------------------------\n",
      " Segment-specific recommendation functions created:\n",
      "  - get_segment_baseline_recommendations()\n",
      "  - get_segment_cf_recommendations()\n",
      "  - get_segment_hybrid_recommendations()\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing Segment-Specific Recommendations\n",
      "------------------------------------------------------------\n",
      "\n",
      "Sample recommendations for one user per segment:\n",
      "\n",
      "Segment 0 (Power Users) - User 3:\n",
      "----------------------------------------\n",
      "  Baseline:\n",
      "    1. Banana\n",
      "    2. Bag of Organic Bananas\n",
      "    3. Organic Hass Avocado\n",
      "  CF:\n",
      "    1. Half And Half Ultra Pasteurized\n",
      "    2. 1% Milkfat Low Fat Vitamin A & D Milk\n",
      "    3. Organic Reduced Fat Milk\n",
      "  Hybrid:\n",
      "    1. Banana\n",
      "    2. Bananas\n",
      "    3. Medium Scarlet Raspberries\n",
      "\n",
      "Segment 1 (Routine Snackers) - User 1:\n",
      "----------------------------------------\n",
      "  Baseline:\n",
      "    1. Banana\n",
      "    2. Sparkling Water Grapefruit\n",
      "    3. Clementines\n",
      "  CF:\n",
      "    1. Unsweetened Terere Yerba Mate\n",
      "    2. Water Mineral\n",
      "    3. Alkalized Water\n",
      "  Hybrid:\n",
      "    1. Popcorn\n",
      "    2. 100 Calorie Healthy Pop Butter Microwave Pop Corn\n",
      "    3. Organic White Cheddar Popcorn\n",
      "\n",
      "Segment 2 (Bulk Shoppers) - User 4:\n",
      "----------------------------------------\n",
      "  Baseline:\n",
      "    1. Banana\n",
      "    2. Bag of Organic Bananas\n",
      "    3. Strawberries\n",
      "  CF:\n",
      "    1. Lo-Carb Energy Drink\n",
      "    2. Rehab Tea + Lemonade + Energy\n",
      "    3. Yogurt Honey Peanut Bar\n",
      "  Hybrid:\n",
      "    1. Lo-Carb Energy Drink\n",
      "    2. Rehab Tea + Lemonade + Energy\n",
      "    3. Yogurt Honey Peanut Bar\n",
      "\n",
      "Segment 3 (Alcohol Enthusiasts) - User 310:\n",
      "----------------------------------------\n",
      "  Baseline:\n",
      "    1. Vodka\n",
      "    2. Pinot Noir\n",
      "    3. Pinot Grigio\n",
      "  CF:\n",
      "    1. Sparkling Water Grapefruit\n",
      "    2. Natural Hard Apple Cider\n",
      "    3. Ready Pac Salad Santa Fe Caesar Bowls\n",
      "  Hybrid:\n",
      "    1. Natural Hard Apple Cider\n",
      "    2. Little Sumpin' Sumpin' Ale\n",
      "    3. Extra IPA Beer\n",
      "\n",
      "Segment 4 (Household Essentials) - User 12:\n",
      "----------------------------------------\n",
      "  Baseline:\n",
      "    1. Banana\n",
      "    2. Strawberries\n",
      "    3. Ultra Soft Facial Tissues\n",
      "  CF:\n",
      "    1. Classic Ocean Whitefish & Tuna Feast Cat Food\n",
      "    2. Select Tender Chicken with Vegetables & Brown Rice Dog Food Recipe\n",
      "    3. Sensitive with Iron Infant Formula\n",
      "  Hybrid:\n",
      "    1. Classic Ocean Whitefish & Tuna Feast Cat Food\n",
      "    2. Select Tender Chicken with Vegetables & Brown Rice Dog Food Recipe\n",
      "    3. Sensitive with Iron Infant Formula\n",
      "\n",
      "============================================================\n",
      "Segment-Specific Models Trained & Tested\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 2.5 - Train Segment-Specific Models\n",
    "# ================================================================\n",
    "#\n",
    "# Train specialized models for each customer segment:\n",
    "#   1. Segment-Specific Baseline (popularity per segment)\n",
    "#   2. Segment-Specific CF (5 SVD models, one per segment)\n",
    "#   3. Segment-Specific Hybrid (segment CF + global CBF)\n",
    "#\n",
    "# Models saved for evaluation in Phase 3 (Notebook 04)\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2.5 - Train Segment-Specific Models\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(f\"Best global model from Section 2.4: {best_model}\")\n",
    "print(\"Training segment-specific models for all approaches...\\n\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Cache Training User Purchase Histories\n",
    "# -----------------------------------------------\n",
    "print(\"-\"*60)\n",
    "print(\"Caching Training User Purchase Histories\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "train_user_purchase_cache = {}\n",
    "all_train_users = train_interactions['user_id'].unique()\n",
    "\n",
    "for user_id in tqdm(all_train_users, desc=\"Building training cache\"):\n",
    "    train_user_purchase_cache[user_id] = set(train_interactions[train_interactions['user_id']==user_id]['product_id'])\n",
    "\n",
    "print(f\" Cached purchase histories for {len(train_user_purchase_cache):,} training users\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 1. Segment-Specific Baseline (Popularity per Segment)\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"1. Creating Segment-Specific Baseline Models\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "segment_popularity = {}\n",
    "\n",
    "for cluster_id in range(5):\n",
    "    segment_name = user_features_clustered[user_features_clustered['cluster']==cluster_id]['segment_name'].iloc[0]\n",
    "    \n",
    "    print(f\"\\nSegment {cluster_id}: {segment_name}\")\n",
    "    \n",
    "    # Filter training data for this segment\n",
    "    segment_users = user_features_clustered[user_features_clustered['cluster']==cluster_id]['user_id'].values\n",
    "    segment_train = train_interactions[train_interactions['user_id'].isin(segment_users)]\n",
    "    \n",
    "    # Calculate popularity within this segment\n",
    "    segment_pop = (segment_train\n",
    "                   .groupby('product_id')\n",
    "                   .size()\n",
    "                   .reset_index(name='frequency')\n",
    "                   .sort_values('frequency', ascending=False))\n",
    "    \n",
    "    segment_popularity[cluster_id] = segment_pop\n",
    "    \n",
    "    print(f\"  Users: {len(segment_users):,}\")\n",
    "    print(f\"  Interactions: {len(segment_train):,}\")\n",
    "    print(f\"  Unique products: {len(segment_pop):,}\")\n",
    "    \n",
    "    # Show top 3 popular products in this segment\n",
    "    top_3 = segment_pop.head(3)\n",
    "    print(f\"  Top 3 popular products:\")\n",
    "    for idx, row in top_3.iterrows():\n",
    "        prod_name = products[products['product_id']==row['product_id']]['product_name'].values[0]\n",
    "        print(f\"    - {prod_name} ({row['frequency']} purchases)\")\n",
    "\n",
    "print(f\"\\n All {len(segment_popularity)} segment-specific baseline models created\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 2. Segment-Specific CF Models\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"2. Training Segment-Specific CF Models\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "from surprise import SVD, Dataset, Reader\n",
    "\n",
    "segment_cf_models = {}\n",
    "\n",
    "for cluster_id in range(5):\n",
    "    segment_name = user_features_clustered[user_features_clustered['cluster']==cluster_id]['segment_name'].iloc[0]\n",
    "    \n",
    "    print(f\"\\nTraining CF model for Segment {cluster_id}: {segment_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Filter training data for this segment\n",
    "    segment_users = user_features_clustered[user_features_clustered['cluster']==cluster_id]['user_id'].values\n",
    "    \n",
    "    # Get segment interactions and calculate ratings (same as global CF)\n",
    "    segment_train = train_interactions[train_interactions['user_id'].isin(segment_users)]\n",
    "    segment_purchase_counts = segment_train.groupby(['user_id', 'product_id']).size().reset_index(name='frequency')\n",
    "    \n",
    "    # Apply log transformation (same as global CF)\n",
    "    segment_purchase_counts['rating'] = np.log1p(segment_purchase_counts['frequency'])  # log(1 + freq)\n",
    "    \n",
    "    # Prepare data for Surprise\n",
    "    segment_cf_data = segment_purchase_counts[['user_id', 'product_id', 'rating']].copy()\n",
    "    \n",
    "    print(f\"  Users: {len(segment_users):,}\")\n",
    "    print(f\"  Interactions: {len(segment_cf_data):,}\")\n",
    "    print(f\"  Rating statistics:\")\n",
    "    print(f\"    Min: {segment_cf_data['rating'].min():.3f}\")\n",
    "    print(f\"    Max: {segment_cf_data['rating'].max():.3f}\")\n",
    "    print(f\"    Mean: {segment_cf_data['rating'].mean():.3f}\")\n",
    "    \n",
    "    # Build dataset (same as global CF)\n",
    "    reader = Reader(rating_scale=(segment_cf_data['rating'].min(), segment_cf_data['rating'].max()))\n",
    "    segment_data = Dataset.load_from_df(segment_cf_data, reader)\n",
    "    segment_trainset = segment_data.build_full_trainset()\n",
    "    \n",
    "    # Train SVD model (same parameters as global CF)\n",
    "    segment_model = SVD(random_state=SEED)\n",
    "    segment_model.fit(segment_trainset)\n",
    "    \n",
    "    # Store model\n",
    "    segment_cf_models[cluster_id] = segment_model\n",
    "    \n",
    "    print(f\"  Model trained\")\n",
    "\n",
    "print(f\"\\n All {len(segment_cf_models)} segment-specific CF models trained\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 3. Segment-Specific Recommendation Functions\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"3. Creating Segment-Specific Recommendation Functions\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "def get_segment_baseline_recommendations(user_id, cluster_id, n=10):\n",
    "    \"\"\"\n",
    "    Segment-specific baseline: Recommend popular products within segment\n",
    "    (excluding user's past purchases)\n",
    "    \n",
    "    Args:\n",
    "        user_id: Target user ID\n",
    "        cluster_id: User's assigned segment\n",
    "        n: Number of recommendations\n",
    "        \n",
    "    Returns:\n",
    "        List of product_ids\n",
    "    \"\"\"\n",
    "    # Get user's purchase history from training cache\n",
    "    user_purchases = train_user_purchase_cache.get(user_id, set())\n",
    "    \n",
    "    # Get segment-specific popularity\n",
    "    segment_pop = segment_popularity[cluster_id]\n",
    "    \n",
    "    # Filter out already purchased items\n",
    "    recommendations = segment_pop[~segment_pop['product_id'].isin(user_purchases)]\n",
    "    \n",
    "    return list(recommendations['product_id'].head(n))\n",
    "\n",
    "def get_segment_cf_recommendations(user_id, cluster_id, n=10, exclude_purchased=True):\n",
    "    \"\"\"\n",
    "    Segment-specific CF recommendations\n",
    "    \n",
    "    Args:\n",
    "        user_id: Target user ID\n",
    "        cluster_id: User's assigned segment\n",
    "        n: Number of recommendations\n",
    "        exclude_purchased: Whether to exclude items user already purchased\n",
    "        \n",
    "    Returns:\n",
    "        List of (product_id, score) tuples\n",
    "    \"\"\"\n",
    "    # Use the segment-specific CF model\n",
    "    segment_model = segment_cf_models[cluster_id]\n",
    "    \n",
    "    # Get all product IDs from training data\n",
    "    all_products = train_interactions['product_id'].unique()\n",
    "    \n",
    "    # Get products user has already purchased\n",
    "    if exclude_purchased:\n",
    "        purchased = train_user_purchase_cache.get(user_id, set())\n",
    "        candidate_products = [p for p in all_products if p not in purchased]\n",
    "    else:\n",
    "        candidate_products = all_products\n",
    "    \n",
    "    # Predict ratings for all candidate products\n",
    "    predictions = []\n",
    "    for product_id in candidate_products:\n",
    "        pred = segment_model.predict(user_id, product_id)\n",
    "        predictions.append((product_id, pred.est))\n",
    "    \n",
    "    # Sort by predicted rating (descending)\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return predictions[:n]\n",
    "\n",
    "def get_segment_hybrid_recommendations(user_id, cluster_id, n=10, cf_weight=0.5, cbf_weight=0.5):\n",
    "    \"\"\"\n",
    "    Segment-specific hybrid: Segment CF + Global CBF\n",
    "    \n",
    "    Args:\n",
    "        user_id: Target user\n",
    "        cluster_id: User's assigned segment\n",
    "        n: Number of recommendations\n",
    "        cf_weight, cbf_weight: Weights for CF and CBF\n",
    "    \n",
    "    Returns:\n",
    "        List of (product_id, score) tuples\n",
    "    \"\"\"\n",
    "    # Get segment-specific CF recommendations\n",
    "    cf_recs = get_segment_cf_recommendations(user_id, cluster_id, n=400, exclude_purchased=True)\n",
    "    \n",
    "    # Get global CBF recommendations (content features don't change per segment)\n",
    "    cbf_recs = get_cbf_recommendations(user_id, n=400)\n",
    "    \n",
    "    # Normalize CF scores\n",
    "    if len(cf_recs) > 0:\n",
    "        cf_scores_dict = {pid: score for pid, score in cf_recs}\n",
    "        cf_min, cf_max = min(cf_scores_dict.values()), max(cf_scores_dict.values())\n",
    "        cf_range = cf_max - cf_min if cf_max > cf_min else 1\n",
    "        cf_scores_norm = {pid: (score - cf_min) / cf_range for pid, score in cf_scores_dict.items()}\n",
    "    else:\n",
    "        cf_scores_norm = {}\n",
    "    \n",
    "    # Normalize CBF scores\n",
    "    if len(cbf_recs) > 0:\n",
    "        cbf_scores_dict = {pid: score for pid, score in cbf_recs}\n",
    "        cbf_min, cbf_max = min(cbf_scores_dict.values()), max(cbf_scores_dict.values())\n",
    "        cbf_range = cbf_max - cbf_min if cbf_max > cbf_min else 1\n",
    "        cbf_scores_norm = {pid: (score - cbf_min) / cbf_range for pid, score in cbf_scores_dict.items()}\n",
    "    else:\n",
    "        cbf_scores_norm = {}\n",
    "    \n",
    "    # Combine\n",
    "    all_products = set(cf_scores_norm.keys()) | set(cbf_scores_norm.keys())\n",
    "    hybrid_scores = {\n",
    "        pid: (cf_weight * cf_scores_norm.get(pid, 0)) + (cbf_weight * cbf_scores_norm.get(pid, 0))\n",
    "        for pid in all_products\n",
    "    }\n",
    "    \n",
    "    return sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "print(\" Segment-specific recommendation functions created:\")\n",
    "print(\"  - get_segment_baseline_recommendations()\")\n",
    "print(\"  - get_segment_cf_recommendations()\")\n",
    "print(\"  - get_segment_hybrid_recommendations()\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Test Segment-Specific Recommendations\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Testing Segment-Specific Recommendations\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Test on a sample user from each segment\n",
    "print(\"\\nSample recommendations for one user per segment:\\n\")\n",
    "\n",
    "for cluster_id in range(5):\n",
    "    segment_name = user_features_clustered[user_features_clustered['cluster']==cluster_id]['segment_name'].iloc[0]\n",
    "    \n",
    "    # Get a sample user from this segment\n",
    "    segment_users = user_features_clustered[user_features_clustered['cluster']==cluster_id]['user_id'].values\n",
    "    sample_user = segment_users[0]\n",
    "    \n",
    "    print(f\"Segment {cluster_id} ({segment_name}) - User {sample_user}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Get recommendations from segment-specific models\n",
    "    baseline_recs = get_segment_baseline_recommendations(sample_user, cluster_id, n=3)\n",
    "    cf_recs = [pid for pid, _ in get_segment_cf_recommendations(sample_user, cluster_id, n=3)]\n",
    "    hybrid_recs = [pid for pid, _ in get_segment_hybrid_recommendations(sample_user, cluster_id, n=3)]\n",
    "    \n",
    "    print(\"  Baseline:\")\n",
    "    for i, pid in enumerate(baseline_recs, 1):\n",
    "        prod_name = products[products['product_id']==pid]['product_name'].values[0]\n",
    "        print(f\"    {i}. {prod_name}\")\n",
    "    \n",
    "    print(\"  CF:\")\n",
    "    for i, pid in enumerate(cf_recs, 1):\n",
    "        prod_name = products[products['product_id']==pid]['product_name'].values[0]\n",
    "        print(f\"    {i}. {prod_name}\")\n",
    "    \n",
    "    print(\"  Hybrid:\")\n",
    "    for i, pid in enumerate(hybrid_recs, 1):\n",
    "        prod_name = products[products['product_id']==pid]['product_name'].values[0]\n",
    "        print(f\"    {i}. {prod_name}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Segment-Specific Models Trained & Tested\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "18186783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Models saved to: ../data/processed/phase2_models.pkl\n",
      " Total size: 2505.5 MB\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# FINAL SAVE CHECKLIST\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "all_data = {\n",
    "    # -----------------------------------------------\n",
    "    # 1. TRAINED MODELS\n",
    "    # -----------------------------------------------\n",
    "    'global_cf_model': svd_model,\n",
    "    'segment_cf_models': segment_cf_models,\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    # 2. FEATURES & DATA STRUCTURES\n",
    "    # -----------------------------------------------\n",
    "    'item_profile': item_profile,\n",
    "    'global_popularity': global_popularity,\n",
    "    'segment_popularity': segment_popularity,\n",
    "    'purchase_counts': purchase_counts,\n",
    "    'train_interactions': train_interactions,\n",
    "    'train_user_purchase_cache': train_user_purchase_cache,\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    # 3. EVALUATION RESULTS\n",
    "    # -----------------------------------------------\n",
    "    'validation_results_df': results_df,\n",
    "    'validation_raw_results': results,\n",
    "    'best_model_name': best_model,\n",
    "    'sample_size': len(val_users),\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    # 4. METADATA\n",
    "    # -----------------------------------------------\n",
    "    'segment_names': {\n",
    "        cluster_id: user_features_clustered[user_features_clustered['cluster']==cluster_id]['segment_name'].iloc[0]\n",
    "        for cluster_id in range(5)\n",
    "    },\n",
    "    'n_clusters': 5,\n",
    "    'n_train_users': train_interactions['user_id'].nunique(),\n",
    "    'n_train_interactions': len(train_interactions),\n",
    "    'n_products': train_interactions['product_id'].nunique(),\n",
    "    'seed': SEED,\n",
    "    \n",
    "}\n",
    "\n",
    "with open('../data/processed/phase2_models.pkl', 'wb') as f:\n",
    "    pickle.dump(all_data, f)\n",
    "\n",
    "print(\" Models saved to: ../data/processed/phase2_models.pkl\")\n",
    "print(f\" Total size: {os.path.getsize('../data/processed/phase2_models.pkl') / 1024 / 1024:.1f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
